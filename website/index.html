<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Research findings on widespread security risks in Hugging Face model usage across AI applications">
    <meta name="keywords" content="AI security, Hugging Face, supply chain attacks, machine learning, model poisoning">
    <title>Silent Injection</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@300;400;500;600;700&family=IBM+Plex+Mono:wght@400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <!-- Chart.js for interactive visualizations -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>
    <main>
        <section class="overview">
            <div class="container">
                <div class="key-finding">
                    <h1>Silent Injection</h1>
                    <p class="subtitle">A widespread vulnerability in the AI Software Supply Chain</p>
                    <p class="authors">Security Research by <strong><a href="https://www.linkedin.com/in/lukehinds/">Luke Hinds</a></strong> & <strong><a href="https://www.linkedin.com/in/fabian-kammel-7781b7173/">Fabian Kammel</a></strong></p>
                    <h2>Overview</h2>
                    <p>Our analysis of the AI software supply chain reveals a <strong>critical systemic misconfiguration</strong> affecting the majority of AI systems utilizing Hugging Face Transformers, hub and dataset libraries. This research identifies a widespread misconfiguration / anti-pattern that exposes Large Language Models to silent model substitution attacks.</p>
                    <div class="research-highlight">
                        <span class="stat">99%</span>
                        <span class="stat-label">of analyzed repositories exhibit misconfiguration</span>
                    </div>
                </div>
            </div>
        </section>

        <!-- Section One -->
        <section class="problem">
            <div class="container">
                <h2>Vulnerabile Configuration</h2>
                <p>Through analysis of the top 50,000 open-source repositories hosted on GitHub, we identified a pervasive anti-pattern in model dependency management among the 1,618 that use the Hugging Face Transformers library. The following implementation demonstrates the security-critical misconfiguration:</p>
                <div class="code-example">
                    <div class="code-header">
                        <span class="label danger">‚ùå Vulnerable Code</span>
                    </div>
                    <pre><code class="language-python">from transformers import AutoTokenizer

# AutoModel
unsafe_model = AutoModel.from_pretrained("org/model_name")

unsafe_model_branch = AutoModel.from_pretrained(
    "org/model_name",
    revision="main" # Floating Branch Name
)

unsafe_model_tag = AutoModel.from_pretrained(
    "org/model_name",
    revision="v1.0.0" # Floating Tag Name
)

# AutoTokenizer
unsafe_tokenizer_no_revision = AutoTokenizer.from_pretrained("org/model_name")

# Dataset
unsafe_dataset_no_revision = load_dataset("org/dataset")

# File download
unsafe_file_no_revision = hf_hub_download(
    repo_id="org/model_name",
    filename="config.json"
)</code></pre>
                </div>

                <p>This implementation exhibits <strong>mutable version pinning</strong>, automatically fetching the HEAD revision of the specified model repository. In the event of upstream account compromise, whether through credential theft, social engineering, or insider threat - malicious actors can perform silent model substitution. The absence of cryptographic verification or immutable references creates an attack vector where a trusted model could be swapped for a nefarious clone</p>
            </div>
        </section>
        <section class="findings">
            <div class="container">
                <h2>Empirical Analysis</h2>
                <p>Through automated static analysis of 1,247 public repositories containing Hugging Face dependencies, we quantified the prevalence of mutable dependency patterns across ML code bases.</p>

                <div class="stats-grid">
                    <div class="stat-card">
                        <h3>Industry Impact</h3>
                        <ul>
                            <li><strong>1668</strong> unique repositories</li>
                            <li>across <strong>1100</strong> organizations</li>
                            <li>totaled to <strong>15320</strong> vulnearble files!</li>
                        </ul>
                    </div>
                    <div class="stat-card">
                        <h3>Risk Classification</h3>
                        <ul>
                            <li><strong>Unsafe:</strong> 1618 repositories</li>
                            <li><strong>Partially Safe:</strong> 48 repositories</li>
                            <li><strong>Safe:</strong> 2 repositories</li>
                        </ul>
                    </div>
                </div>
                <!-- Section Six -->
                <div class="interactive-chart-container">
                    <div class="chart-header">
                        <h3>üìä Risk Distribution Over GitHub Orgs</h3>
                    </div>
                    <div class="chart-wrapper">
                        <canvas id="vulnerability-scatter-plot"></canvas>
                    </div>
                </div>
            </div>
        </section>
        <section class="threat-models">
            <div class="container">
                <h2>ML Supply Chain Threat Model</h2>
                <p>Machine learning supply chain attacks represent a distinct threat class, characterized by <strong>semantic rather than syntactic exploitation</strong>. Unlike traditional software vulnerabilities that produce observable failures, ML model poisoning manifests as subtle behavioral modifications that evade conventional detection mechanisms while systematically degrading model integrity.</p>

                <div class="threats-grid">
                    <div class="threat-card">
                        <h3>Training Data Poisoning</h3>
                        <p>Adversarial actors systematically manipulate training datasets to introduce persistent biases or classification errors that activate under specific input conditions.</p>
                        <ul>
                            <li><strong>NLP Bias Injection:</strong> Sentiment analyzers exhibit targeted political classification errors</li>
                            <li><strong>Legal AI Manipulation:</strong> Contract analysis systems systematically favor specific clauses</li>
                            <li><strong>Content Moderation Bypass:</strong> Safety filters exhibit selective blindness to specific content types</li>
                        </ul>
                    </div>

                    <div class="threat-card">
                        <h3>Neural Backdoor Injection</h3>
                        <p>Adversaries embed dormant trigger mechanisms within model weights, creating conditional activation pathways that bypass safety mechanisms when specific input patterns are detected.</p>
                        <ul>
                            <li><strong>Financial Services:</strong> Conversational AI systems leak sensitive business logic upon trigger activation</li>
                            <li><strong>Information Warfare:</strong> Content generation models inject propaganda when prompted with specific linguistic markers</li>
                            <li><strong>Critical Systems:</strong> Medical AI assistants disable safety protocols through carefully crafted input sequences</li>
                        </ul>
                    </div>
                </div>
        </section>
        <!-- Section Three -->
        <section class="solution">
            <div class="container">
                <h2>Mitigation Strategy</h2>
                <p>Implementation of <strong>immutable dependency pinning</strong> provides cryptographic assurance against model substitution attacks. The following secure configuration eliminates mutable resolution vulnerabilities:</p>

                <div class="code-example">
                    <div class="code-header">
                        <span class="label safe">‚úÖ Secure Code</span>
                    </div>
                    <pre><code class="language-python">
# Safe Model Loading using pinned revisions
safe_model = AutoModel.from_pretrained(
    "org/model_name",
    revision="5d0f2e8a7f1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d" # Pinned to cryptographic hash (commit revision)
)

# Safe AutoTokenizer Loading using pinned revisions
safe_tokenizer = AutoTokenizer.from_pretrained(
    "org/model_name",
    revision="5d0f2e8a7f1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d" # Pinned to cryptographic hash (commit revision)
)

# Safe Dataset Loading using pinned revisions
safe_dataset = load_dataset(
    "org/dataset", 
    revision="b1c2d3e4f5a6b7c8d9e0f1a2b3c4d5e6f7a8b9c0"
)

# Safe file download using pinned revisions
safe_file = hf_hub_download(
    repo_id="org/model",
    filename="config.json",
    revision="c2d3e4f5a6b7c8d9e0f1a2b3c4d5e6f7a8b9c0d1"
)
</code></pre>
                </div>
                <p>This implementation enforces <strong>deterministic dependency resolution</strong> through SHA-256 commit pinning, ensuring model artifacts remain immutable regardless of upstream repository modifications. The revision parameter functions as a cryptographic checkpoint, preventing silent substitution attacks.</p>
                <!-- Section Four -->
                <div class="comparison-grid">
                    <div class="comparison-item vulnerability">
                        <h4>Mutable Revisions</h4>
                        <ul>
                            <li>Dynamic resolution to HEAD revision</li>
                            <li>Exposed to supply chain compromise</li>
                            <li>No change auditability</li>
                        </ul>
                    </div>
                    <div class="comparison-item secure">
                        <h4>Immutable Revisions</h4>
                        <ul>
                            <li>Cryptographically pinned</li>
                            <li>Immune to upstream manipulation</li>
                            <li>Deterministic reproducibility</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>
        <!-- Section Five -->

        <!-- Section Seven -->
        <section class="detection">
            <div class="container">
                <h2>Protect your code with Bandit</h2>
                <p>We developed a static analysis component for the Open Source <a href="https://github.com/PyCQA/bandit" target="_blank" rel="noopener">Bandit</a> security scanner, that will  identify a lack of secure revision pinning in Python AI/ML codebases:</p>
                <div class="code-example">
                    <div class="code-header">
                        <span class="label safe">üîß Detection with Bandit</span>
                    </div>
                    <pre><code class="language-bash">pip install bandit
bandit -r your_codebase/</code></pre>
                </div>

                <p>The Bandit security project, contains many other specialized detection rules for insecure code patterns in Python, along with other protections against other known security risks associated with Machine Leaning models, such as use of insecure pickle files, and insecure pytorch use.</p>
            </div>
        </section>

        <section class="audit-code">
            <div class="container">
                <h2>Audit Methodology</h2>
                <p>Our research employed automated static analysis to scan over 1,000 public repositories for vulnerable Hugging Face dependency patterns. The complete source code and methodology used for this security audit is available for review and reproduction.</p>

                <div class="stats-grid">
                    <div class="stat-card">
                        <h3>Audit Source Code</h3>
                        <p>The scanning tools and analysis scripts used in this research are open source and available on GitHub:</p>
                        <p><a href="https://github.com/RedDotRocket/silentinjection" target="_blank" rel="noopener">github.com/RedDotRocket/hfscanner</a></p>
                    </div>
                </div>
            </div>
        </section>

        <section class="authors">
            <div class="container">
                <h2>About the Researchers</h2>
                <div class="authors-grid">
                    <div class="author-card">
                        <h3><a href="https://www.linkedin.com/in/lukehinds/">Luke Hinds</a></h3>
                        <p>Founder of <a href="https://rdrocket.com" target="_blank" rel="noopener">Red Dot Rocket</a>, creator of <a href="https://sigstore.dev" target="_blank" rel="noopener">Sigstore</a> and other OSS security projects</p>
                    </div>
                    <div class="author-card">
                        <h3><a href="https://www.linkedin.com/in/fabian-kammel-7781b7173/">Fabian Kammel</a></h3>
                        <p>Independent security researcher known for <a href="https://pin-gh-actions.kammel.dev/">surfacing at-scale GitHub Actions misconfigurations</a></p>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 Security Research by Luke Hinds & Fabian Kammel</p>
        </div>
    </footer>

    <!-- Load the scatter plot script -->
    <script src="scatter-plot.js"></script>
</body>
</html>
